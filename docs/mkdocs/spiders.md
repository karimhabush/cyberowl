# Spiders

> Spiders are classes which define how a certain site will be scraped, including how to perform the crawl and how to extract structured data from their pages. In other words, Spiders are the place where you define the custom behaviour for crawling and parsing pages for a particular site.

Cyberowl uses both `Scrapy` spiders and `Selenium` web drivers to collect data from different sources.

* Scrapy is used for static content.
* Selenium is used for dynamically generated content.

---

::: cyberowl.spiders.cisa_spider

---

::: cyberowl.spiders.cert_fr_spider

---

::: cyberowl.spiders.ma_cert_spider

---

::: cyberowl.spiders.ibmcloud_spider

---

::: cyberowl.spiders.vigilance_spider

---

::: cyberowl.spiders.vuldb_spider

---

::: cyberowl.spiders.zdi_spider

---
